import streamlit as st
from dotenv import load_dotenv
import os
import json
from pathlib import Path
from loguru import logger
from datetime import datetime
from typing import Any, List, Mapping, Optional, Dict
from pydantic import Field
import boto3
from langchain_core.documents import Document
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.prompts import PromptTemplate
from langchain_core.language_models import BaseChatModel
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import (AIMessage, BaseMessage, ChatGeneration, ChatResult, HumanMessage, SystemMessage)
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


def main():
    # .env íŒŒì¼ ë¡œë“œ
    load_dotenv()

    st.set_page_config(page_title="agi+", page_icon=":bulb:")

    st.title("_Agile Goal Innovation :blue[agi+]_ :bulb:")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    if st.session_state.conversation is None:
        # RAG ì¤€ë¹„
        if Path("./vector-store").joinpath("index.faiss").exists():
            vector = vector_store(None)
            if vector:
                st.session_state.conversation = chin_conversation(vector)
                if st.session_state.conversation is None:
                    st.error("ëŒ€í™” ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            json_data = get_json_file()
            if json_data:
                document = convert_data(json_data)
                if document:
                    chunk = text_chunk(document)
                    if chunk:
                        vector = vector_store(chunk)
                        if vector:
                            st.session_state.conversation = chin_conversation(vector)
                            if st.session_state.conversation is None:
                                st.error("ëŒ€í™” ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [
            {
                "role": "assistant",
                "content": "ì•ˆë…•í•˜ì„¸ìš”! Agi+ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.\ní˜‘ì—…íˆ´ Agitì— ì €ì¥ëœ ë°ì´í„°ì™€ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, ê³¼ê±° íˆìŠ¤í† ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œ í•´ê²°ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ ìˆœì„œë¡œ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ë ¤ í•©ë‹ˆë‹¤. ğŸ˜Š \n1. ë¨¼ì €, í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œë‚˜ ì°¾ê³ ì í•˜ëŠ” íˆìŠ¤í† ë¦¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n2. ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ê¸°ë¡ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³ , í•„ìš” ì‹œ ì¶”ê°€ ì •ë³´ íƒìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n4. ìµœì¢…ì ìœ¼ë¡œ ë‹µë³€ì„ í™•ì •í•˜ì—¬ ë¬¸ì œ í•´ê²°ì„ ë•ìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì œë¥¼ ë„ì™€ë“œë¦´ê¹Œìš”?"
            }
        ]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": query})

        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            chain = st.session_state.conversation

            if chain is None:
                logger.error("ëŒ€í™” ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                response = "ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨ í•´ì£¼ì„¸ìš”."
                st.error(response)
            else:
                with st.spinner("ì—…ë¬´ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ëŠ” ì¤‘ì´ì—ìš”..."):
                    try:
                        # ì…ë ¥ ê²€ì¦
                        if not query.strip():
                            response = "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
                        else:
                            # Chain ì‹¤í–‰
                            result = chain({"question": query})
                            # logger.info(result["source_documents"])

                            # ì‘ë‹µ ì²˜ë¦¬
                            response = result.get('answer', '')
                            source_document = result.get('source_documents', '')
                            if not response:
                                logger.error("Chainì—ì„œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                response = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                st.error(response)
                            else:
                                st.subheader("ë‹µë³€")
                                st.markdown(response)
                                # st.subheader("ì°¸ê³  ìë£Œ")
                                # for doc in source_document:
                                #     st.write(doc.page_content)
                                #     st.write(doc.metadata["url"])
                                #     st.write("=================")

                    except Exception as e:
                        logger.error(f"Chain ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
                        response = "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        st.error(response)

            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            st.session_state.messages.append({"role": "assistant", "content": response})


# json íŒŒì¼
@st.cache_data
def get_json_file():
    root_dir = Path("./data")  # íƒìƒ‰í•  ë£¨íŠ¸ ë””ë ‰í† ë¦¬
    data = []
    count = 0
    # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì•ˆì˜ ëª¨ë“  í´ë” íƒìƒ‰
    for file_path in root_dir.rglob("*"):
        logger.info(file_path.name)
        for json_file in file_path.rglob("*"):
            try:
                # json íŒŒì¼ ì˜¤í”ˆ
                with open(json_file, "r", encoding="utf-8") as file:
                    # json íŒŒì¼ ë¡œë“œ
                    loader = json.load(file)
                    for item in loader:
                        data.append(item)
                        count += 1
            except FileNotFoundError:
                print("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except json.JSONDecodeError:
                print("JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    logger.info(f"json íŒŒì¼ì—ì„œ {count}ê°œì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    return data


# ë°ì´í„° ë³€í™˜í•˜ê¸°(ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ -> ê°ì²´ ë¦¬ìŠ¤íŠ¸)
def convert_data(data):
    document = []
    for item in data:
        try:
            ts = datetime.fromtimestamp(item['ts']).strftime('%Y-%m-%d %H:%M:%S')
            agit_url = f"https://comento.agit.io/g/{item['group_id']}/wall/{item['id']}" if item[
                'is_parent'] else f"https://comento.agit.io/g/{item['group_id']}/wall/{item['first_thread_id']}#comment_panel_{item['id']}"
            convert = Document(
                # í…ìŠ¤íŠ¸ ë¶„í• ì´ ì ìš©ë˜ëŠ” ë°ì´í„°
                page_content=item['user_message'],
                # ë¶„í• ëœ ë°ì´í„°ì˜ ì •ë³´
                metadata={
                    'id': item['id'],
                    'parent_id': item['first_thread_id'],
                    'writer': item['actor_nickname'],
                    'type': 'ê¸€' if item['is_parent'] else 'ëŒ“ê¸€',
                    'url': agit_url,
                    'ts': ts
                }
            )
            document.append(convert)
        except Exception as e:
            logger.error(f"ë°ì´í„° ë³€í™˜í•˜ê¸°(ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ -> ê°ì²´ ë¦¬ìŠ¤íŠ¸) ì¤‘ ì˜¤ë¥˜ {str(e)}")
    return document


# í…ìŠ¤íŠ¸ ë¶„í• (ì²­í¬)
def text_chunk(data, chunk_save_path=Path("chunk-store"), is_save=False):
    try:
        if is_save is False and chunk_save_path.joinpath("text_chunks.json").exists():
            with open(chunk_save_path.joinpath("text_chunks.json"), "r", encoding="utf-8") as file:
                chunks = json.load(file)
        else:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=2000,
                chunk_overlap=200,
                length_function=len,
                separators=[
                    "\n\n",  # ë‹¨ë½ êµ¬ë¶„
                    "\n",  # ì¤„ë°”ê¿ˆ
                    ".",  # ì˜¨ì 
                    "!",  # ëŠë‚Œí‘œ
                    "?",  # ë¬¼ìŒí‘œ
                    ",",  # ì‰¼í‘œ
                    " ",  # ê³µë°±
                    ""  # ë¬¸ì ë‹¨ìœ„
                ]
            )
            chunks = text_splitter.split_documents(data)
            # ì €ì¥
            with open(chunk_save_path.joinpath("text_chunks.json"), 'w', encoding='utf-8') as file:
                json.dump([chunk.page_content for chunk in chunks], file, ensure_ascii=False, indent=2)
            logger.info(f"í…ìŠ¤íŠ¸ ì²­í¬ê°€ {chunk_save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        return chunks
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¶„í• (ì²­í¬) ì¤‘ ì˜¤ë¥˜ {str(e)}")


# ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ
def vector_store(data=None, vector_save_path=Path("./vector-store"), is_save=False):
    try:
        # ë²¡í„° ì €ì¥ì†Œ ë¡œì»¬ íŒŒì¼ í™•ì¸
        if is_save is False and vector_save_path.joinpath("index.faiss").exists():
            vector = FAISS.load_local(
                str(vector_save_path),
                embeddings=HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask"),
                allow_dangerous_deserialization=True  # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼ì—ì„œë§Œ True ì„¤ì •
            )
        else:
            # ì„ë² ë”©
            embeddings = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )

            # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            vector = FAISS.from_documents(data, embeddings)
            # ë²¡í„° ì €ì¥ì†Œ ë¡œì»¬ ì €ì¥
            vector.save_local(str(vector_save_path))

            logger.info("ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ì €ì¥ ì™„ë£Œ")
        return vector
    except Exception as e:
        logger.error(f"ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ì €ì¥ ì¤‘ ì˜¤ë¥˜ {str(e)}")


def chin_conversation(data):
    bedrock_client = boto3.client(
        service_name=os.getenv("SERVICE_NAME"),
        region_name=os.getenv("REGION_NAME"),
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    )

    # LLM ëª¨ë¸
    llm = ClaudeChatModel(
        bedrock_client=bedrock_client,
        model_name="anthropic.claude-3-5-sonnet-20241022-v2:0",
        max_tokens=2048,
        temperature=0.0
    )

    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ìˆ˜ì •
    memory = ConversationBufferMemory(
        memory_key='chat_history',
        return_messages=True,
        output_key='answer',
        input_key='question'
    )

    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template("""
    ì´ì „ ëŒ€í™” ë‚´ìš©: {chat_history}
í˜„ì¬ ì§ˆë¬¸: {question}

ìœ„ ëŒ€í™” íë¦„ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ ë…ë¦½ì ì´ê³  ì™„ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ë³€í™˜ëœ ì§ˆë¬¸:
    """)

    question_prompt = PromptTemplate.from_template("""
    question
1. ì •ì˜ : ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸, í˜„ì¬ì˜ ì—…ë¬´ë¥¼ í•´ê²° í˜¹ì€ ì§„í–‰í•˜ê¸° ìœ„í•œ ëª©ì í•˜ì—ì„œ ê³¼ê±°ì˜ ì—…ë¬´ íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ê³ ì í•˜ëŠ” ë‚´ìš©
2. ë³€ìˆ˜ : {question}

context
1. ì •ì˜ : ì‚¬ìš©ìê°€ í•´ê²°, í™•ì¸í•˜ê³ ì í•˜ëŠ” ì—…ë¬´ìƒì˜ íˆìŠ¤í† ë¦¬ë¥¼ í•™ìŠµí•œ ë°ì´í„°ì—ì„œ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ë‚´ìš©ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ ê²ƒ
2. ë³€ìˆ˜ : {context}

ì—­í• 
1. ë„ˆëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ questionì„ ê¸°ë°˜ìœ¼ë¡œ ì œê³µëœ contextë¥¼ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•˜ëŠ” ì±—ë´‡ì„.
2. ì´ ë•Œ, ì œê³µëœ contextê°€ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•˜ë„ë¡ questionì˜ ë‚´ìš©ê³¼ ìœ ì‚¬ë„ê°€ ë†’ì•„ì•¼ í•¨.

ë‹µë³€
1. ì‚¬ìš©ìê°€ ì…ë ¥í•œ questionì„ í™•ì¸í•˜ê³ , í‚¤ì›Œë“œ ë¶„ì„ ë° ë¬¸ì œ ìƒí™©ì„ ì •ì˜
2. ë¶„ì„ëœ í‚¤ì›Œë“œ ë° ë¬¸ì œ ìƒí™©ì— ê¸°ë°˜í•´ ìœ ì‚¬ë„ ë†’ì€ contextë¥¼ íƒìƒ‰í•˜ê³ , ì‚¬ìš©ìì—ê²Œ ì´ë¥¼ ì œì‹œ
    """)

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=data.as_retriever(search_type="mmr", search_kwargs={"k": 10}),
        memory=memory,
        condense_question_prompt=CONDENSE_QUESTION_PROMPT,
        combine_docs_chain_kwargs={"prompt": question_prompt},
        return_source_documents=True,
        verbose=True
    )

    return chain


class ClaudeChatModel(BaseChatModel):
    """
    Amazon Bedrockì˜ Claude ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì±„íŒ… ëª¨ë¸ í´ë˜ìŠ¤
    BaseChatModelì„ ìƒì†ë°›ì•„ êµ¬í˜„
    """

    # í´ë˜ìŠ¤ ì†ì„± ì •ì˜
    client: Any = Field(default=None)  # Bedrock í´ë¼ì´ì–¸íŠ¸ ê°ì²´
    model_name: str = Field(default="anthropic.claude-3-5-sonnet-20241022-v2:0")  # ì‚¬ìš©í•  Claude ëª¨ë¸ ë²„ì „
    max_tokens: int = Field(default=2048)  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
    temperature: float = Field(default=0.0)  # ìƒì„± í…ìŠ¤íŠ¸ì˜ ë¬´ì‘ìœ„ì„± ì •ë„ (0.0: ê²°ì •ì , 1.0: ë§¤ìš° ì°½ì˜ì )

    def __init__(self, bedrock_client, **data):
        """
        ì´ˆê¸°í™” í•¨ìˆ˜
        Args:
            bedrock_client: AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ê°ì²´
            **data: ì¶”ê°€ ì„¤ì • ë§¤ê°œë³€ìˆ˜
        """
        super().__init__(**data)
        self.client = bedrock_client

    def _generate(
            self,
            messages: List[BaseMessage],
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any
    ) -> ChatResult:
        """
        ë©”ì‹œì§€ë¥¼ ë°›ì•„ Claude APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ

        Args:
            messages: ì…ë ¥ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            stop: ìƒì„±ì„ ì¤‘ë‹¨í•  í† í° ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
            run_manager: ì½œë°± ê´€ë¦¬ì (ì˜µì…˜)
            **kwargs: ì¶”ê°€ í‚¤ì›Œë“œ ì¸ì

        Returns:
            ChatResult: ìƒì„±ëœ ì‘ë‹µì„ í¬í•¨í•˜ëŠ” ê²°ê³¼ ê°ì²´
        """

        # ì…ë ¥ ë©”ì‹œì§€ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if not messages:
            logger.warning("No messages to process")
            return ChatResult(generations=[ChatGeneration(
                message=AIMessage(content="No input provided"),
                generation_info={"finish_reason": "stop"}
            )])

        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì¶”ì¶œ
        last_message = messages[-1]

        # ë©”ì‹œì§€ íƒ€ì…ì— ë”°ë¥¸ ë‚´ìš© ì¶”ì¶œ
        if isinstance(last_message, (HumanMessage, SystemMessage, AIMessage)):
            message_content = last_message.content
        else:
            logger.warning(f"Unsupported message type: {type(last_message)}")
            message_content = str(last_message.content)

        # Claude API ìš”ì²­ ë³¸ë¬¸ êµ¬ì„±
        body = {
            "anthropic_version": "bedrock-2023-05-31",  # Claude API ë²„ì „
            "max_tokens": self.max_tokens,  # ìµœëŒ€ í† í° ìˆ˜
            "temperature": self.temperature,  # ì˜¨ë„ ì„¤ì •
            "messages": [
                {
                    "role": "user",  # ì‚¬ìš©ì ì—­í• ë¡œ ë©”ì‹œì§€ ì „ì†¡
                    "content": [
                        {
                            "type": "text",
                            "text": message_content
                        }
                    ]
                }
            ]
        }

        try:
            # API ìš”ì²­ ë¡œê¹…
            logger.info(f"Sending request to Claude API with body: {json.dumps(body)}")

            # Bedrock API í˜¸ì¶œ
            completion = self.client.invoke_model(
                modelId=self.model_name,
                body=json.dumps(body),
                accept="application/json",
                contentType="application/json",
            )

            # ì‘ë‹µ íŒŒì‹±
            response = json.loads(completion["body"].read())
            logger.info(f"Received response from Claude API: {json.dumps(response)}")

            # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ì²˜ë¦¬
            if "content" in response and len(response["content"]) > 0:
                response_content = response["content"][0]["text"]
            elif "completion" in response:
                response_content = response["completion"]
            else:
                logger.error(f"Unexpected response structure: {json.dumps(response)}")
                response_content = "ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            # ì²˜ë¦¬ëœ ì‘ë‹µ ë‚´ìš© ë¡œê¹…
            logger.info(f"Processed response content: {response_content}")

            # ChatGeneration ê°ì²´ ìƒì„± ë° ë°˜í™˜
            chat_generation = ChatGeneration(
                message=AIMessage(content=response_content),
                generation_info={"finish_reason": "stop"}
            )

            return ChatResult(generations=[chat_generation])

        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹… ë° ì˜ˆì™¸ ì „íŒŒ
            logger.error(f"Claude API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    @property
    def _llm_type(self) -> str:
        """
        LLM íƒ€ì…ì„ ë°˜í™˜í•˜ëŠ” í”„ë¡œí¼í‹°
        Returns:
            str: LLM íƒ€ì… ì‹ë³„ì
        """
        return "bedrock_claude"

    @property
    def _identifying_params(self) -> dict:
        """
        ëª¨ë¸ì˜ ì‹ë³„ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í”„ë¡œí¼í‹°
        Returns:
            dict: ëª¨ë¸ ì‹ë³„ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        return {
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }


if __name__ == '__main__':
    main()
