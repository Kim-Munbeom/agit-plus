# Python ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st
import tiktoken
import json
import boto3
from pathlib import Path
from typing import Any, List, Mapping, Optional, Dict
from datetime import datetime
import os
from pathlib import Path
from comento_func import bedrock_client

# ë¡œê¹…
from loguru import logger

# Pydantic
from pydantic import BaseModel, Field

# Langchain Core
from langchain_core.prompts import PromptTemplate
from langchain_core.retrievers import BaseRetriever
from langchain_core.memory import BaseMemory  # BaseMemoryë§Œ import
from langchain_core.messages import (AIMessage, HumanMessage, SystemMessage, BaseMessage)
from langchain_core.language_models import BaseChatModel
from langchain_core.documents import Document
from langchain_core.callbacks import CallbackManagerForChainRun
from langchain_core.outputs import ChatResult, ChatGeneration

# Langchain
from langchain.chains import ConversationalRetrievalChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory  # ì—¬ê¸°ì„œ import

# Langchain Community
from langchain_community.document_loaders import (PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader)
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.callbacks import get_openai_callback
from langchain_community.chat_message_histories import StreamlitChatMessageHistory


def main():
    st.set_page_config(page_title="agi+", page_icon=":bulb:")

    st.title("_Agile Goal Innovation :blue[agi+]_ :bulb:")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    # main í•¨ìˆ˜ ë‚´ì— ì¶”ê°€

    if st.session_state.conversation is None:
        data = load_data("data_back")
        if data:
            documents = convert_to_documents(data)
            if documents:
                text_chunks = get_text_chunks(documents)
                if text_chunks:
                    vectorstore = get_vectorstore(text_chunks)
                    if vectorstore:
                        st.session_state.conversation = get_conversation_chain(vectorstore)
                        if st.session_state.conversation is None:
                            st.error("ëŒ€í™” ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [
            {
                "role": "assistant",
                "content": "ì•ˆë…•í•˜ì„¸ìš”! Agi+ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.\ní˜‘ì—…íˆ´ Agitì— ì €ìž¥ëœ ë°ì´í„°ì™€ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, ê³¼ê±° ížˆìŠ¤í† ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œ í•´ê²°ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ì•„ëž˜ì™€ ê°™ì€ ìˆœì„œë¡œ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ë ¤ í•©ë‹ˆë‹¤. ðŸ˜Š \n1. ë¨¼ì €, í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œë‚˜ ì°¾ê³ ìž í•˜ëŠ” ížˆìŠ¤í† ë¦¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n2. ìž…ë ¥í•˜ì‹  ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ê¸°ë¡ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³ , í•„ìš” ì‹œ ì¶”ê°€ ì •ë³´ íƒìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n4. ìµœì¢…ì ìœ¼ë¡œ ë‹µë³€ì„ í™•ì •í•˜ì—¬ ë¬¸ì œ í•´ê²°ì„ ë•ìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì œë¥¼ ë„ì™€ë“œë¦´ê¹Œìš”?"
            }
        ]
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    history = StreamlitChatMessageHistory(key="chat_messages")

    # Chat logic
    if query := st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": query})

        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            chain = st.session_state.conversation

            if chain is None:
                logger.error("ëŒ€í™” ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                response = "ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŽ˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨ í•´ì£¼ì„¸ìš”."
                st.error(response)
            else:
                with st.spinner("ì—…ë¬´ ížˆìŠ¤í† ë¦¬ë¥¼ ì°¾ëŠ” ì¤‘ì´ì—ìš”..."):
                    try:
                        # ìž…ë ¥ ê²€ì¦
                        if not query.strip():
                            response = "ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”."
                        else:
                            # Chain ì‹¤í–‰
                            result = chain({"question": query})
                            logger.info(f"Chain result: {result}")

                            # ì‘ë‹µ ì²˜ë¦¬
                            response = result.get('answer', '')
                            if not response:
                                logger.error("Chainì—ì„œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                response = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                st.error(response)
                            else:
                                st.markdown(response)

                    except Exception as e:
                        logger.error(f"Chain ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
                        response = "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        st.error(response)

            # ë©”ì‹œì§€ ížˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            st.session_state.messages.append({"role": "assistant", "content": response})


def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)


@st.cache_data
def load_data(data_dir: str) -> List[dict]:
    current_dir = Path.cwd()
    logger.info(f"í˜„ìž¬ ìž‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")

    data = []
    data_dir_path = Path(data_dir)
    logger.info(f"ì°¾ìœ¼ë ¤ëŠ” ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ: {data_dir_path.absolute()}")

    if not data_dir_path.exists():
        st.error(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir_path}")
        logger.error(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_dir_path}")
        return data

    # users.json íŒŒì¼ ì²˜ë¦¬
    user_file = data_dir_path / "users.json"
    logger.info(f"users.json íŒŒì¼ ê²½ë¡œ: {user_file}")

    if user_file.exists():
        try:
            with open(user_file, 'r', encoding='utf-8') as file:
                users_data = json.load(file)
                st.write(f"users.jsonì—ì„œ {len(users_data)}ëª…ì˜ ì‚¬ìš©ìž ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                for user in users_data:
                    processed_item = {
                        "type": "user",
                        "user_message": f"""
                        ì´ë¦„: {user.get('nickname', '')}
                        ì•„ì§€íŠ¸ID: {user.get('agit_id', '')}
                        ì†Œì†: {user.get('profile', {}).get('affiliation', '')}
                        ì´ë©”ì¼: {user.get('email', '')}
                        ì†Œê°œ: {user.get('profile', {}).get('introduction', '')}
                        ìœ„ì¹˜: {user.get('profile', {}).get('location', '')}
                        ì—°ë½ì²˜: {user.get('profile', {}).get('phone_number', '')}
                        """.strip(),
                        "raw_data": user
                    }
                    data.append(processed_item)
        except Exception as e:
            st.error(f"users.json íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"users.json íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # data.json íŒŒì¼ ì²˜ë¦¬
    data_file = data_dir_path / "data.json"
    logger.info(f"data.json íŒŒì¼ ê²½ë¡œ: {data_file}")

    if data_file.exists():
        try:
            with open(data_file, 'r', encoding='utf-8') as file:
                posts_data = json.load(file)
                processed_count = 0

                def format_timestamp(ts):
                    if ts:
                        return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
                    return ''

                def process_attachments(files, images):
                    attachments = []
                    if files:
                        for file in files:
                            attachments.append(f"""
                            íŒŒì¼: {file.get('name', '')}
                            í˜•ì‹: {file.get('format', '')}
                            í¬ê¸°: {file.get('size', 0)} bytes
                            ì—…ë¡œë“œ ì‹œê°„: {format_timestamp(file.get('created_at'))}
                            """.strip())

                    if images:
                        for image in images:
                            attachments.append(f"""
                            ì´ë¯¸ì§€: {image.get('name', '')}
                            í˜•ì‹: {image.get('format_type', '')}
                            í¬ê¸°: {image.get('width', '')}x{image.get('height', '')}
                            ì—…ë¡œë“œ ì‹œê°„: {format_timestamp(image.get('created_at'))}
                            """.strip())

                    return attachments

                def process_post(post):
                    # ê¸°ë³¸ ë©”ì‹œì§€ ë‚´ìš© êµ¬ì„±
                    message_content = f"""
                    ìž‘ì„±ìž: {post.get('actor_nickname', 'ìµëª…')}
                    ë‚´ìš©: {post.get('user_message', '')}
                    ìž‘ì„±ì‹œê°„: {format_timestamp(post.get('ts'))}
                    """

                    # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ê°€
                    attachments = process_attachments(post.get('files', []), post.get('images', []))
                    if attachments:
                        message_content += "\nì²¨ë¶€íŒŒì¼:\n" + "\n".join(attachments)

                    # ë°˜ì‘ ì •ë³´ ì¶”ê°€
                    reactions = post.get('reactions', {})
                    if reactions:
                        message_content += f"\në°˜ì‘: ðŸ‘ {reactions.get('like_count', 0)} ðŸ‘Ž {reactions.get('dislike_count', 0)}"

                    # ìš”ì²­ ì •ë³´ ì¶”ê°€
                    request = post.get('request')
                    if request:
                        message_content += f"""
                        \nìš”ì²­ ì •ë³´:
                        ìƒíƒœ: {request.get('status')}
                        ìƒì„±ì‹œê°„: {format_timestamp(request.get('created_at'))}
                        """

                    return {
                        "type": "post",
                        "post_content": message_content.strip(),
                        "metadata": {
                            "post_id": post.get('id'),
                            "group_id": post.get('group_id'),
                            "actor_id": post.get('actor_id'),
                            "actor_nickname": post.get('actor_nickname'),
                            "timestamp": post.get('ts'),
                            "is_parent": post.get('is_parent'),
                            "thread_info": {
                                "first_thread_id": post.get('first_thread_id'),
                                "last_thread_id": post.get('last_thread_id'),
                                "threads_count": post.get('threads_count')
                            },
                            "has_files": bool(post.get('files')),
                            "has_images": bool(post.get('images')),
                            "has_reactions": bool(post.get('reactions')),
                            "has_request": bool(post.get('request'))
                        },
                        "raw_data": post
                    }

                # ê²Œì‹œê¸€ ë°ì´í„° ì²˜ë¦¬
                if isinstance(posts_data, list):
                    for post_group in posts_data:
                        if isinstance(post_group, list):
                            for post in post_group:
                                if isinstance(post, dict):
                                    processed_post = process_post(post)
                                    data.append(processed_post)
                                    processed_count += 1

                logger.info(f"data.jsonì—ì„œ {processed_count}ê°œì˜ ê²Œì‹œê¸€ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
                st.write(f"data.jsonì—ì„œ {processed_count}ê°œì˜ ê²Œì‹œê¸€ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            st.error(f"data.json íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"data.json íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # ìµœì¢… ë°ì´í„° ìˆ˜ ì¶œë ¥
    if data:
        st.write(f"ì´ {len(data)}ê°œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        logger.info(f"ì´ {len(data)}ê°œì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    else:
        st.warning("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        logger.warning("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    return data


def convert_to_documents(data: List[dict]) -> List[Document]:
    documents = []
    for item in data:
        if item['type'] == 'user':
            doc = Document(
                page_content=item['user_message'],
                metadata={'source': 'user_data', 'type': 'user'}
            )
        else:
            # general íƒ€ìž…ì˜ ë°ì´í„°ëŠ” ì „ì²´ ë‚´ìš©ì„ JSON ë¬¸ìžì—´ë¡œ ë³€í™˜
            doc = Document(
                page_content=json.dumps(item, ensure_ascii=False, indent=2),
                metadata={'source': 'general_data', 'type': 'general'}
            )
        documents.append(doc)
    return documents


def get_text_chunks(data, save_path="text_chunks.json"):
    # í…ìŠ¤íŠ¸ ì²­í¬ íŒŒì¼ì´ ì¡´ìž¬í•˜ë©´ ë¡œë“œ
    if os.path.exists(save_path):
        with open(save_path, 'r', encoding='utf-8') as f:
            chunk_contents = json.load(f)
        logger.info(f"í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ {save_path}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return [Document(page_content=content) for content in chunk_contents]

    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len
    )
    chunks = text_splitter.split_documents(data)

    # ì €ìž¥
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump([chunk.page_content for chunk in chunks], f, ensure_ascii=False, indent=2)
    logger.info(f"í…ìŠ¤íŠ¸ ì²­í¬ê°€ {save_path}ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return chunks


def get_vectorstore(text_chunks, save_path="vectorstore.faiss"):
    save_path = Path(save_path)  # pathlib ì‚¬ìš©
    if not save_path.parent.exists():
        save_path.parent.mkdir(parents=True, exist_ok=True)

    # ë²¡í„° ì €ìž¥ì†Œ íŒŒì¼ì´ ì¡´ìž¬í•˜ë©´ ë¡œë“œ
    if save_path.exists():
        try:
            vectordb = FAISS.load_local(
                str(save_path),
                embeddings=HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask"),
                allow_dangerous_deserialization=True  # ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” íŒŒì¼ì—ì„œë§Œ True ì„¤ì •
            )
            logger.info(f"ë²¡í„° ì €ìž¥ì†Œë¥¼ {save_path}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return vectordb
        except Exception as e:
            logger.error(f"ë²¡í„° ì €ìž¥ì†Œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return None

    # í…ìŠ¤íŠ¸ ì²­í¬ê°€ ë¹„ì–´ ìžˆëŠ” ê²½ìš°
    if not text_chunks:
        logger.error("í…ìŠ¤íŠ¸ ì²­í¬ê°€ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
        return None

    try:
        # ìž„ë² ë”© ì´ˆê¸°í™”
        embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # ë²¡í„° ì €ìž¥ì†Œ ìƒì„±
        vectordb = FAISS.from_documents(text_chunks, embeddings)

        # ë¡œì»¬ ì €ìž¥
        vectordb.save_local(save_path)
        logger.info(f"ë²¡í„° ì €ìž¥ì†Œê°€ {save_path}ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        return vectordb
    except Exception as e:
        logger.error(f"ë²¡í„° ì €ìž¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return None


def get_conversation_chain(vectorstore):
    if vectorstore is None:
        logger.error("vectorstoreê°€ Noneìž…ë‹ˆë‹¤.")
        return None

    try:
        llm = ClaudeChatModel(
            bedrock_client=bedrock_client,
            model_name="anthropic.claude-3-5-sonnet-20241022-v2:0",
            max_tokens=2048,
            temperature=0.0
        )

        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ìˆ˜ì •
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            return_messages=True,
            output_key='answer',
            input_key='question'
        )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template("""
Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.

Follow Up Input: {question}

Standalone question:""")

        qa_prompt = PromptTemplate.from_template("""
You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know. DO NOT try to make up an answer.

{context}

Question: {question}

Answer in Korean:""")

        # ì²´ì¸ ìƒì„±
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4}),
            memory=memory,
            condense_question_prompt=CONDENSE_QUESTION_PROMPT,
            combine_docs_chain_kwargs={"prompt": qa_prompt},
            return_source_documents=True,
            verbose=True
        )

        return chain

    except Exception as e:
        logger.error("ëŒ€í™” ì²´ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", str(e))
        return None


class CustomConversationalRetrievalChain(ConversationalRetrievalChain):
    @classmethod
    def from_llm(
            cls,
            llm: BaseChatModel,
            retriever: BaseRetriever,
            memory: Optional[BaseMemory] = None,
            **kwargs
    ) -> "CustomConversationalRetrievalChain":
        """LLMìœ¼ë¡œë¶€í„° ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í´ëž˜ìŠ¤ ë©”ì„œë“œ"""

        # ê¸°ë³¸ ì²´ì¸ ìƒì„±
        chain = super().from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            **kwargs
        )

        # ì»¤ìŠ¤í…€ ì†ì„± ì„¤ì •
        chain.llm = llm
        return chain

    def _get_docs(self, question: str, inputs: Dict[str, Any]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            docs = self.retriever.invoke(question)
            return self._reduce_tokens_below_limit(docs)
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []

    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """ì²´ì¸ ì‹¤í–‰ ë©”ì„œë“œ"""
        question = inputs["question"]
        chat_history = inputs.get("chat_history", [])

        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs = self._get_docs(question, inputs)
        if not docs:
            return {
                "question": question,
                "answer": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "source_documents": [],
                "chat_history": chat_history
            }

        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = self._format_documents(docs)

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_prompt(question, context)

        try:
            messages = [
                SystemMessage(
                    content="You are a helpful assistant that provides accurate information based on the given documents."
                ),
                HumanMessage(content=prompt)
            ]

            response = self.llm.generate([messages])

            if response.generations and response.generations[0]:
                answer = response.generations[0][0].text
            else:
                answer = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            return {
                "question": question,
                "answer": answer,
                "source_documents": docs,
                "chat_history": chat_history + [
                    HumanMessage(content=question),
                    AIMessage(content=answer)
                ]
            }

        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "question": question,
                "answer": "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "source_documents": docs,
                "chat_history": chat_history
            }

    def _create_prompt(self, question: str, context: str) -> str:
        return f"""
question
1. ì •ì˜ : ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ í”„ë¡¬í”„íŠ¸, í˜„ìž¬ì˜ ì—…ë¬´ë¥¼ í•´ê²° í˜¹ì€ ì§„í–‰í•˜ê¸° ìœ„í•œ ëª©ì í•˜ì—ì„œ ê³¼ê±°ì˜ ì—…ë¬´ ížˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ê³ ìž í•˜ëŠ” ë‚´ìš©
2. ë³€ìˆ˜ : {question}

context
1. ì •ì˜ : ì‚¬ìš©ìžê°€ í•´ê²°, í™•ì¸í•˜ê³ ìž í•˜ëŠ” ì—…ë¬´ìƒì˜ ížˆìŠ¤í† ë¦¬ë¥¼ í•™ìŠµí•œ ë°ì´í„°ì—ì„œ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ë‚´ìš©ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ ê²ƒ
2. ë³€ìˆ˜ : {context}

ì—­í• 
1. ë„ˆëŠ” ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ questionì„ ê¸°ë°˜ìœ¼ë¡œ ì œê³µëœ contextë¥¼ ì‚¬ìš©ìžì—ê²Œ ì „ë‹¬í•˜ëŠ” ì±—ë´‡ìž„.
2. ì´ ë•Œ, ì œê³µëœ contextê°€ ì‚¬ìš©ìžì—ê²Œ ìœ ìš©í•˜ë„ë¡ questionì˜ ë‚´ìš©ê³¼ ìœ ì‚¬ë„ê°€ ë†’ì•„ì•¼ í•¨.

ë‹µë³€
1. ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ questionì„ í™•ì¸í•˜ê³ , í‚¤ì›Œë“œ ë¶„ì„ ë° ë¬¸ì œ ìƒí™©ì„ ì •ì˜
2. ë¶„ì„ëœ í‚¤ì›Œë“œ ë° ë¬¸ì œ ìƒí™©ì— ê¸°ë°˜í•´ ìœ ì‚¬ë„ ë†’ì€ contextë¥¼ íƒìƒ‰í•˜ê³ , ì‚¬ìš©ìžì—ê²Œ ì´ë¥¼ ì œì‹œ
ì œì‹œí•  ë•Œì—ëŠ” ë°˜ë“œì‹œ 3ê°œ ì´ìƒì˜ contextë¥¼ ì œì‹œí•´ì•¼ í•¨.
3. ê°ê°ì˜ context ì œì‹œí•  ë•Œì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒì˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•¨.
[
1. í”„ë¡œì íŠ¸ëª…(í˜¹ì€ ì—…ë¬´ì£¼ì œ) :
2. í”„ë¡œì íŠ¸ ë‚´ìš©(í˜¹ì€ ì—…ë¬´ë‚´ìš©) :
3. agit link : 
]
"""


def _format_documents(self, docs: List[Document]) -> str:
    """ë¬¸ì„œ í¬ë§·íŒ… ë©”ì„œë“œ"""
    formatted_docs = []
    for i, doc in enumerate(docs, 1):
        content = doc.page_content
        source = doc.metadata.get('source', 'Unknown')
        doc_type = doc.metadata.get('type', 'Unknown')
        formatted_docs.append(
            f"ë¬¸ì„œ {i} (ì¶œì²˜: {source}, ìœ í˜•: {doc_type}):\n{content}\n"
        )
    return "\n\n".join(formatted_docs)


class ClaudeChatModel(BaseChatModel):
    client: Any = Field(default=None)
    model_name: str = Field(default="anthropic.claude-3-5-sonnet-20241022-v2:0")
    max_tokens: int = Field(default=2048)
    temperature: float = Field(default=0.0)

    def __init__(self, bedrock_client, **data):
        super().__init__(**data)
        self.client = bedrock_client

    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> ChatResult:
        formatted_messages = []

        for message in messages:
            if isinstance(message, (HumanMessage, SystemMessage)):
                formatted_messages.append({
                    "role": "user",
                    "content": message.content
                })
            elif isinstance(message, AIMessage):
                formatted_messages.append({
                    "role": "assistant",
                    "content": message.content
                })

        if not formatted_messages:
            logger.warning("No messages to process")
            return ChatResult(generations=[ChatGeneration(
                message=AIMessage(content="No input provided"),
                generation_info={"finish_reason": "stop"}
            )])

        body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": formatted_messages[-1]["content"]
                        }
                    ]
                }
            ]
        }

        try:
            logger.info(f"Sending request to Claude API with body: {json.dumps(body)}")
            completion = self.client.invoke_model(
                modelId=self.model_name,
                body=json.dumps(body),
                accept="application/json",
                contentType="application/json",
            )

            response = json.loads(completion["body"].read())
            logger.info(f"Received response from Claude API: {json.dumps(response)}")

            # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ì²˜ë¦¬
            if "content" in response and len(response["content"]) > 0:
                response_content = response["content"][0]["text"]
            elif "completion" in response:
                response_content = response["completion"]
            else:
                logger.error(f"Unexpected response structure: {json.dumps(response)}")
                response_content = "ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            logger.info(f"Processed response content: {response_content}")

            # ChatGeneration ê°ì²´ ìƒì„±
            chat_generation = ChatGeneration(
                message=AIMessage(content=response_content),
                generation_info={"finish_reason": "stop"}
            )

            return ChatResult(generations=[chat_generation])

        except Exception as e:
            logger.error(f"Claude API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    @property
    def _llm_type(self) -> str:
        return "bedrock_claude"

    @property
    def _identifying_params(self) -> dict:
        return {
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }


if __name__ == '__main__':
    main()